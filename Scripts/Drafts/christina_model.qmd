---
title: "christina_model"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nflfastR)
library(caret)
library(keras)
library(keras3)
library(neuralnet)
library(nnet)
library(dplyr)
library(tensorflow)
library(fastDummies)
library(reticulate)

set.seed(666)
```

```{r}
games <- nflreadr::load_schedules(2021:2024)
pbp <- load_pbp(2021:2024)
performances <- pbp %>%
  group_by(game_id) %>%
  slice_tail(n=1)%>%
  select(game_id, total_home_epa, total_home_rush_epa, total_home_pass_epa, total_home_comp_air_epa, total_home_raw_air_epa, total_home_comp_yac_epa, total_home_comp_air_wpa, total_home_comp_yac_wpa, total_home_pass_wpa, total_home_raw_air_wpa, total_home_rush_wpa, total_home_raw_yac_epa, total_home_raw_yac_wpa, total_away_comp_air_epa, total_away_comp_air_wpa, total_away_comp_yac_epa, total_away_comp_yac_wpa, total_away_epa, total_away_pass_epa, total_away_pass_wpa, total_away_raw_air_epa, total_away_raw_air_wpa, total_away_raw_yac_epa, total_away_raw_yac_wpa, total_away_rush_epa, total_away_rush_wpa)

games <- games %>% left_join(performances, by = 'game_id')

games <- games %>%
  select(-old_game_id, -nfl_detail_id, -pfr, -pff, -espn, -ftn, -away_qb_id, -home_qb_id, -away_qb_name, -home_qb_name, -away_coach, -home_coach, -referee, -stadium, -stadium_id, -location) 

games$home_win <- ifelse(games$result > 0, 1,0)

games <- games %>% 
  arrange(home_team)
```

```{r}
nfl_data <- load_pbp(2021:2024)
```

```{r}
## Logistic Regression

# Select and preprocess relevant features
logistic_data <- nfl_data %>%
  filter(!is.na(epa)) %>%
  select(
    result, epa, home_wp, away_wp, quarter_seconds_remaining,
    roof, play_type
  )

# One-hot encode categorical variables
logistic_data <- logistic_data %>%
  mutate(
    roof = as.factor(roof),
    play_type = as.factor(play_type)
  ) %>%
  dummy_cols(select_columns = c("roof", "play_type"), remove_first_dummy = TRUE)

# Split into training and testing sets
set.seed(123)
split <- createDataPartition(logistic_data$result, p = 0.8, list = FALSE)
train_data <- logistic_data[split, ]
test_data <- logistic_data[-split, ]

# Extract predictors and target variable
x_train <- train_data %>% select(-result)
y_train <- as.numeric(train_data$result > 0)  # Binary win/loss
x_test <- test_data %>% select(-result)
y_test <- as.numeric(test_data$result > 0)

```

```{r}
# Fit the logistic regression model
logistic_model <- glm(y_train ~ ., data = cbind(y_train, x_train), family = "binomial")

# Summary of the model
summary(logistic_model)

```

```{r}
# Predict probabilities
pred_probs <- predict(logistic_model, newdata = x_test, type = "response")

# Convert probabilities to binary predictions
predicted_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion matrix
library(caret)
confusionMatrix(
  factor(predicted_classes),
  factor(y_test),
  positive = "1"
)

# Evaluate accuracy
accuracy <- mean(predicted_classes == y_test)
cat("Test Accuracy:", accuracy, "\n")

```

```{r}
# Exponentiate coefficients to get odds ratios
exp(coef(logistic_model))

# Confidence intervals for odds ratios
exp(confint(logistic_model))

```

```{r}

## NN

# Select relevant features
nfl_data <- nfl_data %>%
  filter(!is.na(epa)) %>%
  select(
    game_id, play_type, epa, home_team, away_team, roof, weather,
    home_wp, away_wp, quarter_seconds_remaining, posteam, defteam, result
  )

# One-hot encode categorical variables
nfl_data <- nfl_data %>%
  mutate(
    roof = as.factor(roof),
    play_type = as.factor(play_type),
    posteam = as.factor(posteam),
    defteam = as.factor(defteam)
  ) %>%
  dummy_cols(select_columns = c("roof", "play_type", "posteam", "defteam"))

# Split into training and testing sets
set.seed(123)
split <- createDataPartition(nfl_data$result, p = 0.8, list = FALSE)
train_data <- nfl_data[split, ]
test_data <- nfl_data[-split, ]

# Scale numerical features
scale_features <- function(df) {
  num_cols <- sapply(df, is.numeric)
  df[, num_cols] <- scale(df[, num_cols])
  return(df)
}
train_data <- scale_features(train_data)
test_data <- scale_features(test_data)

# Extract predictors and target variable
x_train <- train_data %>% select(-result)
y_train <- train_data$result
x_test <- test_data %>% select(-result)
y_test <- test_data$result

```

```{r, eval=FALSE}
# Convert data to matrices
x_train <- as.matrix(x_train)
x_test <- as.matrix(x_test)
y_train <- as.numeric(y_train > 0)  # Convert to binary (win/loss)
y_test <- as.numeric(y_test > 0)

# Define the neural network model
model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = "sigmoid")  # Binary classification

# Compile the model
model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# Train the model
# Does not run, keep showing `reticulate::py_last_error()` on my end
#history <- model %>% fit(
#  x_train, y_train,
#  epochs = 50,
#  batch_size = 32,
#  validation_split = 0.2,
#  verbose = 1
#)

```
